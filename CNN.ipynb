{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0AHJ-a_r5unA"
      },
      "source": [
        "### Code for plotting the keras model graph\n",
        "Uncomment -> run -> comment out -> restart runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "I5WEOZdbL_rL"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !pip install pydot\n",
        "# !apt-get install graphviz\n",
        "# !sed -i 's/def _check_pydot()/def _check__pydot()/g' /usr/local/lib/python3.6/dist-packages/keras/utils/vis_utils.py\n",
        "# !sed -i 's/_check_pydot()/#_check_pydot()/g' /usr/local/lib/python3.6/dist-packages/keras/utils/vis_utils.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-sBgr9Xr8gA1"
      },
      "source": [
        "### [Cheatsheet](https://drive.google.com/open?id=1iWhJiMT9pgWqYA_3-iRyvQ1DwlhV3hGdR-pinZiiHfk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D9GySzxhOLJ0"
      },
      "source": [
        "### **Disclaimer**:\n",
        "\n",
        "These functions were developed for educational purposes.\n",
        "\n",
        "The main idea is to keep the code as simple and clean as possible for someone to get the structure of each model easily.\n",
        "\n",
        "For more configurable implementations of these models please refer to other available github repositories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ppyNTxqV6XXv"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "buDTHaDL7bqK"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, Conv3D, DepthwiseConv2D, SeparableConv2D, Conv3DTranspose\n",
        "from tensorflow.keras.layers import Flatten, MaxPool2D, AvgPool2D, GlobalAvgPool2D, UpSampling2D, BatchNormalization\n",
        "from tensorflow.keras.layers import Concatenate, Add, Dropout, ReLU, Lambda, Activation, LeakyReLU, PReLU\n",
        "\n",
        "from IPython.display import SVG\n",
        "from tensorflow.keras.utils import model_to_dot\n",
        "\n",
        "from time import time\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mbbOdmRM_Adq"
      },
      "source": [
        "## AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XpOhEDia_334"
      },
      "outputs": [],
      "source": [
        "def alexnet(input_shape, n_classes):\n",
        "    input = Input(input_shape)\n",
        "    \n",
        "    x = Conv2D(96, 5, strides=1, padding='same', activation='relu')(input)  # Reduced kernel size\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPool2D(2, strides=2)(x)  # Reduced pooling size\n",
        "    \n",
        "    x = Conv2D(256, 3, padding='same', activation='relu')(x)  # Reduced kernel size\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPool2D(2, strides=2)(x)\n",
        "    \n",
        "    x = Conv2D(384, 3, strides=1, padding='same', activation='relu')(x)\n",
        "    x = Conv2D(384, 3, strides=1, padding='same', activation='relu')(x)\n",
        "    x = Conv2D(256, 3, strides=1, padding='same', activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPool2D(2, strides=2)(x)  # Works better now\n",
        "    \n",
        "    x = Flatten()(x)\n",
        "    x = Dense(1024, activation='relu')(x)  # Reduced FC layer size\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    \n",
        "    output = Dense(n_classes, activation='softmax')(x)\n",
        "    \n",
        "    model = Model(input, output)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8KzDeXAB_Co0"
      },
      "source": [
        "## VGG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "6lwZD4p_7l02"
      },
      "outputs": [],
      "source": [
        "def vgg(input_shape, n_classes):\n",
        "  \n",
        "  input = Input(input_shape)\n",
        "  \n",
        "  x = Conv2D(64, 3, padding='same', activation='relu')(input)\n",
        "  x = Conv2D(64, 3, padding='same', activation='relu')(x)\n",
        "  x = MaxPool2D(2, strides=2, padding='same')(x)\n",
        "  \n",
        "  x = Conv2D(128, 3, padding='same', activation='relu')(x)\n",
        "  x = Conv2D(128, 3, padding='same', activation='relu')(x)\n",
        "  x = MaxPool2D(2, strides=2, padding='same')(x)\n",
        "  \n",
        "  x = Conv2D(256, 3, padding='same', activation='relu')(x)\n",
        "  x = Conv2D(256, 3, padding='same', activation='relu')(x)\n",
        "  x = Conv2D(256, 3, padding='same', activation='relu')(x)\n",
        "  x = MaxPool2D(2, strides=2, padding='same')(x)\n",
        "  \n",
        "  x = Conv2D(512, 3, padding='same', activation='relu')(x)\n",
        "  x = Conv2D(512, 3, padding='same', activation='relu')(x)\n",
        "  x = Conv2D(512, 3, padding='same', activation='relu')(x)\n",
        "  x = MaxPool2D(2, strides=2, padding='same')(x)\n",
        "  \n",
        "  x = Conv2D(512, 3, padding='same', activation='relu')(x)\n",
        "  x = Conv2D(512, 3, padding='same', activation='relu')(x)\n",
        "  x = Conv2D(512, 3, padding='same', activation='relu')(x)\n",
        "  x = MaxPool2D(2, strides=2, padding='same')(x)\n",
        "  \n",
        "  x = Flatten()(x)\n",
        "  x = Dense(4096, activation='relu')(x)\n",
        "  x = Dense(4096, activation='relu')(x)\n",
        "  \n",
        "  output = Dense(n_classes, activation='softmax')(x)\n",
        "  \n",
        "  model = Model(input, output)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QmxPPvv6_Ekt"
      },
      "source": [
        "## Inception"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Lo98U9x_GUR5"
      },
      "outputs": [],
      "source": [
        "def googlenet(input_shape, n_classes):\n",
        "  \n",
        "  def inception_block(x, f):\n",
        "    t1 = Conv2D(f[0], 1, activation='relu')(x)\n",
        "    \n",
        "    t2 = Conv2D(f[1], 1, activation='relu')(x)\n",
        "    t2 = Conv2D(f[2], 3, padding='same', activation='relu')(t2)\n",
        "    \n",
        "    t3 = Conv2D(f[3], 1, activation='relu')(x)\n",
        "    t3 = Conv2D(f[4], 5, padding='same', activation='relu')(t3)\n",
        "    \n",
        "    t4 = MaxPool2D(3, 1, padding='same')(x)\n",
        "    t4 = Conv2D(f[5], 1, activation='relu')(t4)\n",
        "    \n",
        "    output = Concatenate()([t1, t2, t3, t4])\n",
        "    return output\n",
        "  \n",
        "  \n",
        "  input = Input(input_shape)\n",
        "  \n",
        "  x = Conv2D(64, 7, strides=2, padding='same', activation='relu')(input)\n",
        "  x = MaxPool2D(3, strides=2, padding='same')(x)\n",
        "  \n",
        "  x = Conv2D(64, 1, activation='relu')(x)\n",
        "  x = Conv2D(192, 3, padding='same', activation='relu')(x)\n",
        "  x = MaxPool2D(3, strides=2)(x)\n",
        "  \n",
        "  x = inception_block(x, [64, 96, 128, 16, 32, 32])\n",
        "  x = inception_block(x, [128, 128, 192, 32, 96, 64])\n",
        "  x = MaxPool2D(3, strides=2, padding='same')(x)\n",
        "  \n",
        "  x = inception_block(x, [192, 96, 208, 16, 48, 64])\n",
        "  x = inception_block(x, [160, 112, 224, 24, 64, 64])\n",
        "  x = inception_block(x, [128, 128, 256, 24, 64, 64])\n",
        "  x = inception_block(x, [112, 144, 288, 32, 64, 64])\n",
        "  x = inception_block(x, [256, 160, 320, 32, 128, 128])\n",
        "  x = MaxPool2D(3, strides=2, padding='same')(x)\n",
        "\n",
        "  x = inception_block(x, [256, 160, 320, 32, 128, 128])\n",
        "  x = inception_block(x, [384, 192, 384, 48, 128, 128])\n",
        "  \n",
        "  x = AvgPool2D(7, strides=1)(x)\n",
        "  x = Dropout(0.4)(x)\n",
        "  \n",
        "  x = Flatten()(x)\n",
        "  output = Dense(n_classes, activation='softmax')(x)\n",
        "  \n",
        "  model = Model(input, output)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Rx6nnBXy_GpA"
      },
      "source": [
        "## MobileNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "7cX_4MhW_lyl"
      },
      "outputs": [],
      "source": [
        "def mobilenet(input_shape, n_classes):\n",
        "  \n",
        "  def mobilenet_block(x, f, s=1):\n",
        "    x = DepthwiseConv2D(3, strides=s, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    \n",
        "    x = Conv2D(f, 1, strides=1, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    return x\n",
        "    \n",
        "    \n",
        "  input = Input(input_shape)\n",
        "\n",
        "  x = Conv2D(32, 3, strides=2, padding='same')(input)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = ReLU()(x)\n",
        "\n",
        "  x = mobilenet_block(x, 64)\n",
        "  x = mobilenet_block(x, 128, 2)\n",
        "  x = mobilenet_block(x, 128)\n",
        "\n",
        "  x = mobilenet_block(x, 256, 2)\n",
        "  x = mobilenet_block(x, 256)\n",
        "\n",
        "  x = mobilenet_block(x, 512, 2)\n",
        "  for _ in range(5):\n",
        "    x = mobilenet_block(x, 512)\n",
        "\n",
        "  x = mobilenet_block(x, 1024, 2)\n",
        "  x = mobilenet_block(x, 1024)\n",
        "  \n",
        "  x = GlobalAvgPool2D()(x)\n",
        "  \n",
        "  output = Dense(n_classes, activation='softmax')(x)\n",
        "  \n",
        "  model = Model(input, output)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zBTuyT22_l-3"
      },
      "source": [
        "## ShuffleNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "RfosZPYf_oFe"
      },
      "outputs": [],
      "source": [
        "def shufflenet(input_shape, n_classes, g=8):\n",
        "  channels = 384, 769, 1536\n",
        "  repetitions = 3, 7, 3\n",
        "  \n",
        "  def ch_shuffle(x, g):\n",
        "#     1 2 3 4 5 6 7 8 9 -reshape-> 1 2 3 -permute dims-> 1 4 7 -reshape-> 1 4 7 2 5 8 3 6 9\n",
        "#                                  4 5 6                 2 5 8\n",
        "#                                  7 8 9                 3 6 9\n",
        "    \n",
        "    _, w, h, ch = K.int_shape(x)\n",
        "    ch_g = ch // g\n",
        "    \n",
        "    def shuffle_op(x):\n",
        "      x = K.reshape(x, [-1, w, h, ch_g, g])\n",
        "      x = K.permute_dimensions(x, [0, 1, 2, 4, 3])\n",
        "      x = K.reshape(x, [-1, w, h, ch])\n",
        "      return x\n",
        "    \n",
        "    x = Lambda(shuffle_op)(x)\n",
        "    return x\n",
        "    \n",
        "  \n",
        "  def gconv(tensor, ch, g):\n",
        "    _, _, _, in_ch = K.int_shape(tensor)\n",
        "    ch_g = in_ch // g\n",
        "    out_ch = ch // g\n",
        "    group = []\n",
        "    \n",
        "    for i in range(g):\n",
        "#       x = tensor[:, :, :, i*ch_g:(i+1)*ch_g]\n",
        "      x = Lambda(lambda x: x[:, :, :, i*ch_g: (i+1)*ch_g])(tensor)\n",
        "      x = Conv2D(out_ch, 1)(x)\n",
        "      group.append(x)\n",
        "    \n",
        "    x = Concatenate()(group)\n",
        "    return x\n",
        "  \n",
        "  \n",
        "  def shufflenet_block(tensor, ch, s, g):\n",
        "    x = gconv(tensor, ch // 4, g)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x = ch_shuffle(x, g)\n",
        "    x = DepthwiseConv2D(3, strides=s, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = gconv(x, ch if s==1 else ch-K.int_shape(tensor)[-1], g)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    if s == 1:\n",
        "      x = Add()([tensor, x])\n",
        "    else:\n",
        "      avg = AvgPool2D(3, strides=2, padding='same')(tensor)\n",
        "      x = Concatenate()([avg, x])\n",
        "    \n",
        "    output = ReLU()(x)\n",
        "    return output\n",
        "  \n",
        "  \n",
        "  def stage(x, ch, r, g):\n",
        "    x = shufflenet_block(x, ch, 2, g)\n",
        "    \n",
        "    for i in range(r):\n",
        "      x = shufflenet_block(x, ch, 1, g)\n",
        "      \n",
        "    return x\n",
        "  \n",
        "  input = Input(input_shape)\n",
        "  \n",
        "  x = Conv2D(24, 3, strides=2, padding='same')(input)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = ReLU()(x)\n",
        "  x = MaxPool2D(3, strides=2, padding='same')(x)\n",
        "  \n",
        "  for ch, r in zip(channels, repetitions):\n",
        "    x = stage(x, ch, r, g)\n",
        "    \n",
        "  x = GlobalAvgPool2D()(x)\n",
        "  output = Dense(n_classes, activation='softmax')(x)\n",
        "  \n",
        "  model = Model(input, output)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kfJ8zng4_oWe"
      },
      "source": [
        "## ResNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "-LbF4hlj_t7m"
      },
      "outputs": [],
      "source": [
        "def resnet(input_shape, n_classes):\n",
        "  \n",
        "  def conv_bn_rl(x, f, k=1, s=1, p='same'):\n",
        "    x = Conv2D(f, k, strides=s, padding=p)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    return x\n",
        "  \n",
        "  \n",
        "  def identity_block(tensor, f):\n",
        "    x = conv_bn_rl(tensor, f)\n",
        "    x = conv_bn_rl(x, f, 3)\n",
        "    x = Conv2D(4*f, 1)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    x = Add()([x, tensor])\n",
        "    output = ReLU()(x)\n",
        "    return output\n",
        "  \n",
        "  \n",
        "  def conv_block(tensor, f, s):\n",
        "    x = conv_bn_rl(tensor, f)\n",
        "    x = conv_bn_rl(x, f, 3, s)\n",
        "    x = Conv2D(4*f, 1)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    shortcut = Conv2D(4*f, 1, strides=s)(tensor)\n",
        "    shortcut = BatchNormalization()(shortcut)\n",
        "    \n",
        "    x = Add()([x, shortcut])\n",
        "    output = ReLU()(x)\n",
        "    return output\n",
        "  \n",
        "  \n",
        "  def resnet_block(x, f, r, s=2):\n",
        "    x = conv_block(x, f, s)\n",
        "    for _ in range(r-1):\n",
        "      x = identity_block(x, f)\n",
        "    return x\n",
        "    \n",
        "  \n",
        "  input = Input(input_shape)\n",
        "  \n",
        "  x = conv_bn_rl(input, 64, 7, 2)\n",
        "  x = MaxPool2D(3, strides=2, padding='same')(x)\n",
        "  \n",
        "  x = resnet_block(x, 64, 3, 1)\n",
        "  x = resnet_block(x, 128, 4)\n",
        "  x = resnet_block(x, 256, 6)\n",
        "  x = resnet_block(x, 512, 3)\n",
        "  \n",
        "  x = GlobalAvgPool2D()(x)\n",
        "  \n",
        "  output = Dense(n_classes, activation='softmax')(x)\n",
        "  \n",
        "  model = Model(input, output)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FLz0AGu4_wEi"
      },
      "source": [
        "## DenseNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "s1yjw8vW-5ge"
      },
      "outputs": [],
      "source": [
        "def densenet(img_shape, n_classes, f=32):\n",
        "  repetitions = 6, 12, 24, 16\n",
        "  \n",
        "  def bn_rl_conv(x, f, k=1, s=1, p='same'):\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x = Conv2D(f, k, strides=s, padding=p)(x)\n",
        "    return x\n",
        "  \n",
        "  \n",
        "  def dense_block(tensor, r):\n",
        "    for _ in range(r):\n",
        "      x = bn_rl_conv(tensor, 4*f)\n",
        "      x = bn_rl_conv(x, f, 3)\n",
        "      tensor = Concatenate()([tensor, x])\n",
        "    return tensor\n",
        "  \n",
        "  \n",
        "  def transition_block(x):\n",
        "    x = bn_rl_conv(x, K.int_shape(x)[-1] // 2)\n",
        "    x = AvgPool2D(2, strides=2, padding='same')(x)\n",
        "    return x\n",
        "  \n",
        "  \n",
        "  input = Input(img_shape)\n",
        "  \n",
        "  x = Conv2D(64, 7, strides=2, padding='same')(input)\n",
        "  x = MaxPool2D(3, strides=2, padding='same')(x)\n",
        "  \n",
        "  for r in repetitions:\n",
        "    d = dense_block(x, r)\n",
        "    x = transition_block(d)\n",
        "  \n",
        "  x = GlobalAvgPool2D()(d)\n",
        "  \n",
        "  output = Dense(n_classes, activation='softmax')(x)\n",
        "  \n",
        "  model = Model(input, output)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "czKlYBNv6O4E"
      },
      "source": [
        "## Xception"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "AH6T1YN24tcm"
      },
      "outputs": [],
      "source": [
        "def xception(input_shape, n_classes):\n",
        "  \n",
        "  def conv_bn(x, f, k, s=1, p='same'):\n",
        "    x = Conv2D(f, k, strides=s, padding=p, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    return x\n",
        "  \n",
        "  \n",
        "  def sep_bn(x, f, k, s=1, p='same'):\n",
        "    x = SeparableConv2D(f, k, strides=s, padding=p, use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    return x\n",
        "  \n",
        "  \n",
        "  def entry_flow(x):\n",
        "    x = conv_bn(x, 32, 3, 2)\n",
        "    x = ReLU()(x)\n",
        "    x = conv_bn(x, 64, 3)\n",
        "    tensor = ReLU()(x)\n",
        "    \n",
        "    x = sep_bn(tensor, 128, 3)\n",
        "    x = ReLU()(x)\n",
        "    x = sep_bn(x, 128, 3)\n",
        "    x = MaxPool2D(3, strides=2, padding='same')(x)\n",
        "    \n",
        "    tensor = conv_bn(tensor, 128, 1, 2)\n",
        "    \n",
        "    x = Add()([tensor, x])\n",
        "    x = ReLU()(x)\n",
        "    x = sep_bn(x, 256, 3)\n",
        "    x = ReLU()(x)\n",
        "    x = sep_bn(x, 256, 3)\n",
        "    x = MaxPool2D(3, strides=2, padding='same')(x)\n",
        "    \n",
        "    tensor = conv_bn(tensor, 256, 1, 2)\n",
        "    \n",
        "    x = Add()([tensor, x])\n",
        "    x = ReLU()(x)\n",
        "    x = sep_bn(x, 728, 3)\n",
        "    x = ReLU()(x)\n",
        "    x = sep_bn(x, 728, 3)\n",
        "    x = MaxPool2D(3, strides=2, padding='same')(x)\n",
        "    \n",
        "    tensor = conv_bn(tensor, 728, 1, 2)\n",
        "    x = Add()([tensor, x])\n",
        "    \n",
        "    return x\n",
        "  \n",
        "  \n",
        "  def middle_flow(tensor):\n",
        "    for _ in range(8):\n",
        "      x = ReLU()(tensor)\n",
        "      x = sep_bn(x, 728, 3)\n",
        "      x = ReLU()(x)\n",
        "      x = sep_bn(x, 728, 3)\n",
        "      x = ReLU()(x)\n",
        "      x = sep_bn(x, 728, 3)\n",
        "\n",
        "      tensor = Add()([tensor, x])\n",
        "    \n",
        "    return tensor\n",
        "  \n",
        "  \n",
        "  def exit_flow(tensor):\n",
        "    x = ReLU()(tensor)\n",
        "    x = sep_bn(x, 728, 3)\n",
        "    x = ReLU()(x)\n",
        "    x = sep_bn(x, 1024, 3)\n",
        "    x = MaxPool2D(3, strides=2, padding='same')(x)\n",
        "    \n",
        "    tensor = conv_bn(tensor, 1024, 1, 2)\n",
        "    \n",
        "    x = Add()([tensor, x])\n",
        "    x = sep_bn(x, 1536, 3)\n",
        "    x = ReLU()(x)\n",
        "    x = sep_bn(x, 2048, 3)\n",
        "    x = ReLU()(x)\n",
        "    x = GlobalAvgPool2D()(x)\n",
        "    x = Dense(n_classes, activation='softmax')(x)\n",
        "  \n",
        "    return x\n",
        "  \n",
        "  \n",
        "  input = Input(input_shape)\n",
        "  \n",
        "  x = entry_flow(input)\n",
        "  x = middle_flow(x)\n",
        "  output = exit_flow(x)\n",
        "  \n",
        "  model = Model(input, output)\n",
        "  \n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7bwiihQfg5PM"
      },
      "source": [
        "## Unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "a_L-XieEg6yT"
      },
      "outputs": [],
      "source": [
        "def unet(input_shape=(572, 572, 1), f=64, steps=4, n_classes=2):\n",
        "  \n",
        "  def downstream(x, f):\n",
        "    x = Conv2D(f, 3, activation='relu')(x)\n",
        "    d = Conv2D(f, 3, activation='relu')(x)\n",
        "    x = MaxPool2D(2, strides=2, padding='same')(d)\n",
        "    return d, x\n",
        "  \n",
        "  \n",
        "  def crop_merge(x, d):\n",
        "    _, xw, xh, _ = K.int_shape(x)\n",
        "    _, dw, dh, _ = K.int_shape(d)\n",
        "    mw, mh = (dw-xw)//2, (dh-xh)//2\n",
        "    \n",
        "    d = Lambda(lambda x: x[:, mw: dw-mw, mh: dh-mh, :])(d)\n",
        "    x = Concatenate()([d, x])\n",
        "    return x\n",
        "    \n",
        "  \n",
        "  def upstream(x, f, d):\n",
        "    x = UpSampling2D()(x)\n",
        "    x = Conv2D(f, 2, padding='same')(x)\n",
        "    x = crop_merge(x, d)\n",
        "    x = Conv2D(f, 3, activation='relu')(x)\n",
        "    x = Conv2D(f, 3, activation='relu')(x)\n",
        "    return x\n",
        "    \n",
        "  \n",
        "  input = Input(input_shape)\n",
        "  x = input\n",
        "  \n",
        "  downsampled = []\n",
        "  for i in range(steps+1):\n",
        "    d, x = downstream(x, f*2**i)\n",
        "    downsampled.append(d)\n",
        "  x = downsampled.pop()\n",
        "  \n",
        "  for i in range(steps-1, -1, -1):\n",
        "    x = upstream(x, f*2**i, downsampled[i])\n",
        "  \n",
        "  output = Conv2D(n_classes, 1)(x)\n",
        "  model = Model(input, output)\n",
        "  \n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x_mKW2Yh_dM6"
      },
      "source": [
        "## SqueezeNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "enUGc9iu_ggU"
      },
      "outputs": [],
      "source": [
        "def squeezenet(input_shape, n_classes):\n",
        "  \n",
        "  def fire(x, fs, fe):\n",
        "    s = Conv2D(fs, 1, activation='relu')(x)\n",
        "    e1 = Conv2D(fe, 1, activation='relu')(s)\n",
        "    e3 = Conv2D(fe, 3, padding='same', activation='relu')(s)\n",
        "    output = Concatenate()([e1, e3])\n",
        "    return output\n",
        "  \n",
        "  \n",
        "  input = Input(input_shape)\n",
        "  \n",
        "  x = Conv2D(96, 7, strides=2, padding='same', activation='relu')(input)\n",
        "  x = MaxPool2D(3, strides=2, padding='same')(x)\n",
        "  \n",
        "  x = fire(x, 16, 64)\n",
        "  x = fire(x, 16, 64)\n",
        "  x = fire(x, 32, 128)\n",
        "  x = MaxPool2D(3, strides=2, padding='same')(x)\n",
        "  \n",
        "  x = fire(x, 32, 128)\n",
        "  x = fire(x, 48, 192)\n",
        "  x = fire(x, 48, 192)\n",
        "  x = fire(x, 64, 256)\n",
        "  x = MaxPool2D(3, strides=2, padding='same')(x)\n",
        "  \n",
        "  x = fire(x, 64, 256)\n",
        "  x = Conv2D(n_classes, 1)(x)\n",
        "  x = GlobalAvgPool2D()(x)\n",
        "  \n",
        "  output = Activation('softmax')(x)\n",
        "  \n",
        "  model = Model(input, output)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PCqote7DVRsv"
      },
      "source": [
        "## YOLO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "HeM_X8b6VW3p"
      },
      "outputs": [],
      "source": [
        "def yolo(input_shape=(448, 448, 3), n_outputs=30):\n",
        "  activation = LeakyReLU(0.1)\n",
        "  \n",
        "  def conv_1_3(x, f1, f2, r=1):\n",
        "    for _ in range(r):\n",
        "      x = Conv2D(f1, 1, padding='same', activation=activation)(x)\n",
        "      x = Conv2D(f2, 3, padding='same', activation=activation)(x)\n",
        "    return x\n",
        "    \n",
        "  \n",
        "  input = Input(input_shape)\n",
        "  \n",
        "  x = Conv2D(64, 7, strides=2, padding='same', activation=activation)(input)\n",
        "  x = MaxPool2D(2, strides=2, padding='same')(x)\n",
        "  \n",
        "  x = Conv2D(192, 3, padding='same', activation=activation)(x)\n",
        "  x = MaxPool2D(2, strides=2, padding='same')(x)\n",
        "  \n",
        "  x = conv_1_3(x, 128, 256)\n",
        "  x = conv_1_3(x, 256, 512)\n",
        "  x = MaxPool2D(2, strides=2, padding='same')(x)\n",
        "  \n",
        "  x = conv_1_3(x, 256, 512, 4)\n",
        "  x = conv_1_3(x, 512, 1024)\n",
        "  x = MaxPool2D(2, strides=2, padding='same')(x)\n",
        "  \n",
        "  x = conv_1_3(x, 512, 1024, 2)\n",
        "  x = Conv2D(1024, 3, padding='same', activation=activation)(x)\n",
        "  x = Conv2D(1024, 3, strides=2, padding='same', activation=activation)(x)\n",
        "  \n",
        "  x = Conv2D(1024, 3, padding='same', activation=activation)(x)\n",
        "  x = Conv2D(1024, 3, padding='same', activation=activation)(x)\n",
        "  \n",
        "  x = Dense(4096, activation=activation)(x)\n",
        "  output = Dense(n_outputs)(x)\n",
        "  \n",
        "  model = Model(input, output)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UY9TNeWjUnye"
      },
      "source": [
        "## RefineNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "AJ6vj_oFUoNc"
      },
      "outputs": [],
      "source": [
        "def refinenet(backbone_model, n_classes=21, tensor_names=('add_3', 'add_7', 'add_13', 'add_16')):\n",
        "  '''\n",
        "  backbone_model: (pretrained, non trainable) model used to extract the features (refinenet's inputs) \n",
        "  tensor_names: list of names of the backbone_model's tensors to be used as inputs (default: names from resnet)\n",
        "  '''\n",
        "  \n",
        "  filters = 512, 256, 256, 256\n",
        "  input = backbone_model.input\n",
        "  \n",
        "  def rcu(tensor, f, conv_shortcut=False):\n",
        "    x = ReLU()(tensor)\n",
        "    x = Conv2D(f, 3, padding='same')(x)\n",
        "    x = ReLU()(tensor)\n",
        "    x = Conv2D(f, 3, padding='same')(x)\n",
        "    \n",
        "    if conv_shortcut:\n",
        "      tensor = Conv2D(f, 3, padding='same')(tensor)\n",
        "      \n",
        "    output = Add()([tensor, x])\n",
        "    return output\n",
        "  \n",
        "  \n",
        "  def fusion_block(x1, x2, f):\n",
        "    x1 = Conv2D(f, 3, padding='same')(x1)\n",
        "    \n",
        "    x2 = Conv2D(f, 3, padding='same')(x2)\n",
        "    x2 = UpSampling2D()(x2)\n",
        "    \n",
        "    output = Add()([x1, x2])\n",
        "    return output\n",
        "  \n",
        "  \n",
        "  \n",
        "  def chained_block(tensor, steps=2):\n",
        "    f = K.int_shape(tensor)[-1]\n",
        "    tensor = ReLU()(tensor)\n",
        "    \n",
        "    x = tensor\n",
        "    for _ in range(steps):\n",
        "      x = MaxPool2D(5, strides=1, padding='same')(x)\n",
        "      x = Conv2D(f, 3, padding='same')(x)\n",
        "      tensor = Add()([tensor, x])\n",
        "    \n",
        "    return tensor\n",
        "  \n",
        "  \n",
        "  def refine_block(x1, x2=None, f=256):\n",
        "    x = rcu(x1, f, True)\n",
        "    x = rcu(x, f)\n",
        "    \n",
        "    if x2 is not None:\n",
        "      x2 = rcu(x2, f, True)\n",
        "      x2 = rcu(x2, f)\n",
        "      x = fusion_block(x, x2, f)\n",
        "    \n",
        "    x = chained_block(x)\n",
        "    \n",
        "    output = rcu(x, f)\n",
        "    return output\n",
        "  \n",
        "  \n",
        "  inputs = [layer.output for layer in backbone_model.layers if layer.name in tensor_names][::-1]\n",
        "  x = None\n",
        "  \n",
        "  for t, f in zip(inputs, filters):\n",
        "    x = refine_block(t, x, f)\n",
        "  \n",
        "  x = rcu(x, filters[-1])\n",
        "  x = rcu(x, filters[-1])\n",
        "  \n",
        "  output = Dense(n_classes, activation='softmax')(x)\n",
        "  model = Model(input, output)\n",
        "  \n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gtXd0xSfUpNQ"
      },
      "source": [
        "## V-Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "9PFw3EHgUrRd"
      },
      "outputs": [],
      "source": [
        "def vnet(input_shape=(128, 128, 64, 1), n_classes=2):\n",
        "  \n",
        "  def down_conv(x, filters):\n",
        "      x = Conv3D(filters=filters, kernel_size=2, strides=2, padding='same')(x)\n",
        "      x = PReLU()(x)\n",
        "      return x\n",
        "\n",
        "\n",
        "  def up_conv(x, filters):\n",
        "      x = Conv3DTranspose(filters=filters, kernel_size=2, strides=2, padding='same')(x)\n",
        "      x = PReLU()(x)\n",
        "      return x\n",
        "\n",
        "\n",
        "  def conv(x, filters):\n",
        "      x = Conv3D(filters=filters, kernel_size=5, strides=1, padding='same')(x)\n",
        "      x = PReLU()(x)\n",
        "      return x\n",
        "\n",
        "\n",
        "  def downstream_block(tensor, f1, f2, r):\n",
        "    x = tensor\n",
        "    for _ in range(r):\n",
        "      x = conv(x, filters=f1)\n",
        "    x = Add()([tensor, x])\n",
        "    forwards.append(x)\n",
        "    x = down_conv(x, filters=f2)\n",
        "    return x\n",
        "\n",
        "\n",
        "  n_filters = 16, 32, 64, 128, 256\n",
        "  repetitions = 1, 2, 3, 3, 3\n",
        "\n",
        "  forwards = []\n",
        "\n",
        "  # downstream (1st-4th block)\n",
        "  input = Input(input_shape)\n",
        "  tensor = input\n",
        "  for r, f1, f2 in zip(repetitions, n_filters, n_filters[1:]):\n",
        "    tensor = downstream_block(tensor, f1, f2, r)\n",
        "\n",
        "  # 5th block\n",
        "  x = tensor\n",
        "  for _ in range(repetitions[-1]):\n",
        "    x = conv(x, filters=n_filters[-1])\n",
        "  x = Add()([tensor, x])\n",
        "  tensor = up_conv(x, filters=n_filters[-1])\n",
        "\n",
        "\n",
        "  def upstream_block(tensor, f1, f2, r):\n",
        "    forward = forwards.pop()\n",
        "    x = Concatenate()([forward, tensor])\n",
        "    for _ in range(r):\n",
        "      x = conv(x, f1)\n",
        "    x = Add()([tensor, x])\n",
        "    x = up_conv(x, filters=f2)\n",
        "    return x\n",
        "\n",
        "\n",
        "  # upstream (6st-8th block)\n",
        "  for r, f1, f2 in zip(repetitions[-2:0:-1], n_filters[-1:0:-1], n_filters[-2:0:-1]):\n",
        "    tensor = upstream_block(tensor, f1, f2, r)\n",
        "\n",
        "  # 9th block\n",
        "  forward = forwards.pop()\n",
        "  x = Concatenate()([forward, tensor])\n",
        "  x = conv(x, filters=n_filters[1])\n",
        "  x = Add()([tensor, x])\n",
        "  output = Conv3D(filters=n_classes, kernel_size=1, activation='softmax')(x)\n",
        "\n",
        "  model = Model(input, output)\n",
        "  \n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M4_wmfYx6Jkg"
      },
      "source": [
        "## Print and plot the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_hT79naf9xN6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1741886999.406018   24895 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1665 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">34,944</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">614,656</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">885,120</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,327,488</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">884,992</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9216</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │    <span style=\"color: #00af00; text-decoration-color: #00af00\">37,752,832</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │    <span style=\"color: #00af00; text-decoration-color: #00af00\">16,781,312</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,097,000</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m96\u001b[0m)     │        \u001b[38;5;34m34,944\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m96\u001b[0m)     │           \u001b[38;5;34m384\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m96\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m614,656\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │         \u001b[38;5;34m1,024\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m384\u001b[0m)    │       \u001b[38;5;34m885,120\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m384\u001b[0m)    │     \u001b[38;5;34m1,327,488\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m884,992\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │         \u001b[38;5;34m1,024\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9216\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │    \u001b[38;5;34m37,752,832\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │    \u001b[38;5;34m16,781,312\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)           │     \u001b[38;5;34m4,097,000\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">62,380,776</span> (237.96 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m62,380,776\u001b[0m (237.96 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">62,379,560</span> (237.96 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m62,379,560\u001b[0m (237.96 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,216</span> (4.75 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,216\u001b[0m (4.75 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "input_shape = 224, 224, 3\n",
        "n_classes = 1000\n",
        "\n",
        "K.clear_session()\n",
        "model = alexnet(input_shape, n_classes)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "7N_s-8hlMXE_"
      },
      "outputs": [],
      "source": [
        "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WNC4vNba_hYw"
      },
      "source": [
        "## Calculate inference time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "qVQLIdGx_kdm"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1741887070.692002   25060 service.cc:148] XLA service 0x723c80005070 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "I0000 00:00:1741887070.692294   25060 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
            "2025-03-13 23:01:10.706623: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "I0000 00:00:1741887070.773193   25060 cuda_dnn.cc:529] Loaded cuDNN version 90701\n",
            "2025-03-13 23:01:10.812929: W external/local_xla/xla/service/gpu/nvptx_compiler.cc:930] The NVIDIA driver's CUDA version is 12.4 which is older than the PTX compiler version 12.8.61. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
            "2025-03-13 23:01:11.258263: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4', 12 bytes spill stores, 12 bytes spill loads\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1741887072.497029   25060 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "0.049177932739257815\n"
          ]
        }
      ],
      "source": [
        "repetitions = 10\n",
        "input = np.random.randn(1, *input_shape)\n",
        "\n",
        "output = model.predict(input)\n",
        "start = time()\n",
        "for _ in range(repetitions):\n",
        "  output = model.predict(input)\n",
        "  \n",
        "print((time() - start) / repetitions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1741888043.391296   26439 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1625 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
            "2025-03-13 23:17:24.414750: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 614400000 exceeds 10% of free system memory.\n",
            "2025-03-13 23:17:24.869624: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 614400000 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1741888046.900935   26529 service.cc:148] XLA service 0x71c430004380 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "I0000 00:00:1741888046.901099   26529 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
            "2025-03-13 23:17:26.950368: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "I0000 00:00:1741888047.182942   26529 cuda_dnn.cc:529] Loaded cuDNN version 90701\n",
            "2025-03-13 23:17:27.251248: W external/local_xla/xla/service/gpu/nvptx_compiler.cc:930] The NVIDIA driver's CUDA version is 12.4 which is older than the PTX compiler version 12.8.61. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
            "2025-03-13 23:17:27.989330: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_813', 236 bytes spill stores, 236 bytes spill loads\n",
            "\n",
            "2025-03-13 23:17:28.226222: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_813', 8 bytes spill stores, 8 bytes spill loads\n",
            "\n",
            "2025-03-13 23:17:28.308861: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_820', 492 bytes spill stores, 388 bytes spill loads\n",
            "\n",
            "2025-03-13 23:17:28.423213: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_813_0', 756 bytes spill stores, 444 bytes spill loads\n",
            "\n",
            "2025-03-13 23:17:28.986213: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_813', 448 bytes spill stores, 716 bytes spill loads\n",
            "\n",
            "2025-03-13 23:17:28.994804: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1027', 44 bytes spill stores, 60 bytes spill loads\n",
            "\n",
            "2025-03-13 23:17:29.148558: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1027', 64 bytes spill stores, 64 bytes spill loads\n",
            "\n",
            "2025-03-13 23:17:29.269712: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_820', 132 bytes spill stores, 132 bytes spill loads\n",
            "\n",
            "2025-03-13 23:17:29.381620: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1027', 56 bytes spill stores, 56 bytes spill loads\n",
            "\n",
            "2025-03-13 23:17:29.857249: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1027', 52 bytes spill stores, 52 bytes spill loads\n",
            "\n",
            "2025-03-13 23:17:29.867542: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1027', 40 bytes spill stores, 40 bytes spill loads\n",
            "\n",
            "2025-03-13 23:17:30.122831: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1027', 588 bytes spill stores, 548 bytes spill loads\n",
            "\n",
            "2025-03-13 23:17:33.325314: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 946.75MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m  3/500\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 47ms/step - accuracy: 0.1072 - loss: 6.1316   "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1741888058.317108   26529 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m499/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.3509 - loss: 2.1360"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-13 23:18:01.535502: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 689.62MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 50ms/step - accuracy: 0.3513 - loss: 2.1338 - val_accuracy: 0.3268 - val_loss: 2.2455\n",
            "Epoch 2/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 45ms/step - accuracy: 0.6109 - loss: 1.0933 - val_accuracy: 0.5870 - val_loss: 1.1397\n",
            "Epoch 3/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 45ms/step - accuracy: 0.7226 - loss: 0.7864 - val_accuracy: 0.6757 - val_loss: 0.9299\n",
            "Epoch 4/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 46ms/step - accuracy: 0.7870 - loss: 0.6009 - val_accuracy: 0.6390 - val_loss: 1.1322\n",
            "Epoch 5/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 46ms/step - accuracy: 0.8318 - loss: 0.4833 - val_accuracy: 0.7485 - val_loss: 0.7870\n",
            "Epoch 6/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 45ms/step - accuracy: 0.8759 - loss: 0.3536 - val_accuracy: 0.7328 - val_loss: 0.8968\n",
            "Epoch 7/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 46ms/step - accuracy: 0.9161 - loss: 0.2479 - val_accuracy: 0.7547 - val_loss: 0.8396\n",
            "Epoch 8/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 47ms/step - accuracy: 0.9376 - loss: 0.1827 - val_accuracy: 0.7307 - val_loss: 1.0262\n",
            "Epoch 9/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 46ms/step - accuracy: 0.9547 - loss: 0.1360 - val_accuracy: 0.7390 - val_loss: 1.0592\n",
            "Epoch 10/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 46ms/step - accuracy: 0.9571 - loss: 0.1238 - val_accuracy: 0.7717 - val_loss: 0.9906\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-13 23:21:31.002293: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_161', 200 bytes spill stores, 200 bytes spill loads\n",
            "\n",
            "2025-03-13 23:21:31.239089: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_168', 100 bytes spill stores, 100 bytes spill loads\n",
            "\n",
            "2025-03-13 23:21:31.273135: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_161_0', 228 bytes spill stores, 228 bytes spill loads\n",
            "\n",
            "2025-03-13 23:21:31.350929: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_161_0', 88 bytes spill stores, 120 bytes spill loads\n",
            "\n",
            "2025-03-13 23:21:31.405577: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_168', 420 bytes spill stores, 420 bytes spill loads\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m310/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7754 - loss: 0.9553"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-13 23:21:35.031582: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_168', 100 bytes spill stores, 100 bytes spill loads\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - accuracy: 0.7753 - loss: 0.9557\n",
            "Test accuracy: 0.7717\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize the data\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train, y_test = to_categorical(y_train, 10), to_categorical(y_test, 10)\n",
        "\n",
        "# Create and compile the model\n",
        "model = alexnet((32, 32, 3), 10)  # Takes the imput size and the categoriesx\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, batch_size=100, epochs=10, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "ConvNets.ipynb",
      "provenance": [],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
